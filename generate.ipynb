{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9202d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# snnTorch Imports\n",
    "import snntorch as snn\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import spikegen\n",
    "from snntorch import surrogate\n",
    "from snntorch import functional as SF\n",
    "\n",
    "# Torch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Python Libary Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "from snntorch import utils\n",
    "import random\n",
    "import configparser\n",
    "\n",
    "# Script Imports\n",
    "from test_config import getHyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957b5768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#network parameters\n",
    "nin = 2\n",
    "nout = 1\n",
    "rand_range_start = 1\n",
    "rand_range_end = 10\n",
    "beta = 0.95\n",
    "spike_grad = surrogate.fast_sigmoid(slope=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aa7036",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EONS:\n",
    "    def __init__(self,params):\n",
    "        self.params = params\n",
    "        self.template_network = None\n",
    "        pass\n",
    "    def make_template_network(self,nin:int,nout:int):\n",
    "        hidden = random.randint(rand_range_start,rand_range_end)\n",
    "        hidden2 = random.randint(rand_range_start,rand_range_end)\n",
    "        hidden3 = random.randint(rand_range_start,rand_range_end)\n",
    "        hidden4 = random.randint(rand_range_start,rand_range_end)\n",
    "        hidden5 = random.randint(rand_range_start,rand_range_end)\n",
    "        hidden6 = random.randint(rand_range_start,rand_range_end)\n",
    "        net = nn.Sequential(nn.Linear(nin,hidden),\n",
    "                            snn.Leaky(beta=beta,spike_grad=spike_grad),\n",
    "                            nn.Linear(hidden,hidden2),\n",
    "                            snn.Leaky(beta=beta,spike_grad=spike_grad),\n",
    "                            nn.Linear(hidden2,hidden3),\n",
    "                            snn.Leaky(beta=beta,spike_grad=spike_grad),\n",
    "                            nn.Linear(hidden3,hidden4),\n",
    "                            snn.Leaky(beta=beta,spike_grad=spike_grad),\n",
    "                            nn.Linear(hidden4,hidden5),\n",
    "                            snn.Leaky(beta=beta,spike_grad=spike_grad),\n",
    "                            nn.Linear(hidden5,hidden6),\n",
    "                            snn.Leaky(beta=beta,spike_grad=spike_grad),\n",
    "                            nn.Linear(hidden6,nout),\n",
    "                            snn.Leaky(beta=beta,spike_grad=spike_grad))\n",
    "        self.template_network = net\n",
    "        return None\n",
    "    \n",
    "    def add_node(self):\n",
    "        \"\"\"\n",
    "        This method adds a neuron to a randomly selected hidden layer and updates the \n",
    "        in_features to the next layer accordingly.\n",
    "        \"\"\"\n",
    "        # Don't do anything if the network didn't initialize properly\n",
    "        if self.template_network is None:\n",
    "            print(\"Template network not initialized\")\n",
    "            return\n",
    "        \n",
    "        # Find indicies for all linear layers\n",
    "        layer_indices = [i for i, layer in enumerate(self.template_network) if isinstance(layer, nn.Linear)]\n",
    "\n",
    "        # Exclude input and output layers\n",
    "        if (len(layer_indices) <= 2):\n",
    "               print(\"Not enough layers to modify (only input/output layers present)\")\n",
    "               return\n",
    "\n",
    "        hidden_layer_indices = layer_indices[1:-1] # Only hidden layers\n",
    "        selected_index = random.choice(hidden_layer_indices) # Select random layer from hidden layer list\n",
    "        next_index = layer_indices[layer_indices.index(selected_index) + 1] # Get the next layer so that we can update the input\n",
    "\n",
    "        # Get the layers\n",
    "        linear = self.template_network[selected_index]\n",
    "        next_linear = self.template_network[next_index]\n",
    "\n",
    "        in_feat= linear.in_features # Keeping in features the same\n",
    "        new_out_feat = linear.out_features + 1 # Adding neuron to the out\n",
    "\n",
    "        # Creating updated hidden layer\n",
    "        new_linear = nn.Linear(in_feat, new_out_feat)\n",
    "\n",
    "        # Ensuring it keeps the same weights and biases\n",
    "        with torch.no_grad():\n",
    "             new_linear.weight[:linear.out_features] = linear.weight # Copies weights from old layer to new one\n",
    "             new_linear.bias[:linear.out_features] = linear.bias # Copies bias value\n",
    "\n",
    "        # Repeat for the next hidden layer. This ensures that the output of prev layer matches input of next layer\n",
    "        next_out = next_linear.out_features\n",
    "        new_next_linear = nn.Linear(new_out_feat, next_out)\n",
    "        with torch.no_grad():\n",
    "            new_next_linear.weight[:, :linear.out_features] = next_linear.weight # Copies only the part of the next layer's weight matrix that connects to the existing neurons\n",
    "            new_next_linear.bias = next_linear.bias # Copies biases for output layer neuron\n",
    "\n",
    "        # Replacing both layers in the sequential\n",
    "        layers = list(self.template_network)\n",
    "        layers[selected_index] = new_linear\n",
    "        layers[next_index] = new_next_linear\n",
    "        \n",
    "        self.template_network = nn.Sequential(*layers)\n",
    "        print(f\"Added a neuron to output of layer {selected_index} and updated input to layer {next_index}\")\n",
    "\n",
    "    def remove_node(self):\n",
    "        \"\"\"\n",
    "        This method removes a neuron from a randomly selected hidden layer.\n",
    "        The in_features of the next layer is updated accordingly.\n",
    "        If there isn't enough neurons in the layer to remove (if # of neurons is <= 1),\n",
    "        the function simply returns and no mutation occurs. \n",
    "        \"\"\"\n",
    "        # Don't do anything if the network didn't initialize properly\n",
    "        if self.template_network is None:\n",
    "            print(\"Template network not initialized\")\n",
    "            return\n",
    "        \n",
    "        # Find indicies for all linear layers\n",
    "        layer_indices = [i for i, layer in enumerate(self.template_network) if isinstance(layer, nn.Linear)]\n",
    "\n",
    "        # Exclude input and output layers\n",
    "        if (len(layer_indices) <= 2):\n",
    "               print(\"Not enough layers to modify (only input/output layers present)\")\n",
    "               return\n",
    "\n",
    "        hidden_layer_indices = layer_indices[1:-1] # Only hidden layers\n",
    "        selected_index = random.choice(hidden_layer_indices) # Select random layer from hidden layer list\n",
    "        next_index = layer_indices[layer_indices.index(selected_index) + 1] # Get the next layer so that we can update the input\n",
    "\n",
    "        # Get the layers\n",
    "        linear = self.template_network[selected_index]\n",
    "        next_linear = self.template_network[next_index]\n",
    "\n",
    "        # Recursively call method until there are more than one neuron in the out features\n",
    "        if (linear.out_features <= 1):\n",
    "            print(f\"Not enough neurons in layer {selected_index}. Cannot remove neuron.\")\n",
    "            self.remove_node()\n",
    "\n",
    "        in_feat= linear.in_features # Keeping in features the same\n",
    "        new_out_feat = linear.out_features - 1 # Removing neuron to the out\n",
    "\n",
    "        # Creating updated hidden layer\n",
    "        new_linear = nn.Linear(in_feat, new_out_feat)\n",
    "\n",
    "        # Ensuring it keeps the same weights and biases\n",
    "        with torch.no_grad():\n",
    "             new_linear.weight[:] = linear.weight[:new_out_feat] # Keep the first N-1 neurons' weights\n",
    "             new_linear.bias[:] = linear.bias[:new_out_feat] # Copies bias values except the one being removed\n",
    "\n",
    "        # Repeat for the next hidden layer. This ensures that the output of prev layer matches input of next layer\n",
    "        next_out = next_linear.out_features\n",
    "        new_next_linear = nn.Linear(new_out_feat, next_out)\n",
    "        with torch.no_grad():\n",
    "            new_next_linear.weight[:, :] = next_linear.weight[:, :new_out_feat] # Copies all output rows, but only the first new_out_feat input columns\n",
    "            new_next_linear.bias = next_linear.bias # Copies biases for output layer neuron\n",
    "\n",
    "        # Replacing both layers in the sequential\n",
    "        layers = list(self.template_network)\n",
    "        layers[selected_index] = new_linear\n",
    "        layers[next_index] = new_next_linear\n",
    "        \n",
    "        self.template_network = nn.Sequential(*layers) # Update network\n",
    "        print(f\"Removed a neuron to output of layer {selected_index} and updated input to layer {next_index}\")\n",
    "\n",
    "    def update_node_param(self):\n",
    "        \"\"\"\n",
    "        This method udpates the weights and bias of one randomly selected neuron in a randomly selected\n",
    "        hidden layer.\n",
    "\n",
    "        NOTE: There are a couple of lines that are commented out. These lines were created for visualization purposes\n",
    "              so that I could see if the neuron parameters were really updating. If you want to see those results,\n",
    "              simply uncomment those lines and uncomment the necessary lines in the cell below this one. \n",
    "        \"\"\"\n",
    "        # Don't do anything if the network didn't initialize properly\n",
    "        if self.template_network is None:\n",
    "            print(\"Template network not initialized\")\n",
    "            return\n",
    "        \n",
    "        # Find indicies for all linear layers\n",
    "        layer_indices = [i for i, layer in enumerate(self.template_network) if isinstance(layer, nn.Linear)]\n",
    "\n",
    "        # Select random layer\n",
    "        selected_index = random.choice(layer_indices) \n",
    "        linear = self.template_network[selected_index]\n",
    "\n",
    "        # Select a random neuron and generate random weights/bias\n",
    "        new_weight = torch.rand(1)\n",
    "        new_bias = torch.rand(1)\n",
    "        neuron_index = random.choice(range(linear.out_features))\n",
    "\n",
    "        #print(\"Newly generated weight and bias\")\n",
    "        #print(new_weight)\n",
    "        #print(new_bias)\n",
    "        \n",
    "        layers = list(self.template_network) # Convert network to list\n",
    "\n",
    "        #old_weights = layers[selected_index].weight[neuron_index].clone() \n",
    "        #old_bias = layers[selected_index].bias[neuron_index].clone() \n",
    "\n",
    "        # Update neuron weights and bias\n",
    "        with torch.no_grad():\n",
    "            layers[selected_index].weight[neuron_index] = new_weight\n",
    "            layers[selected_index].bias[neuron_index] = new_bias\n",
    "\n",
    "        self.template_network = nn.Sequential(*layers) # Updated network\n",
    "\n",
    "        print(f\"Updated neuron {neuron_index} in layer {selected_index}\")\n",
    "\n",
    "        #return selected_index, neuron_index, old_weights, old_bias \n",
    "\n",
    "    def add_edge(self):\n",
    "        \"\"\"\n",
    "        Dynamically add a synapse (new connection) between two randomly selected layers in the network.\n",
    "        This method adds a new nn.Linear layer between two existing layers.\n",
    "        \"\"\"\n",
    "        if self.template_network is None:\n",
    "            print(\"Template network not initialized\")\n",
    "            return\n",
    "        \n",
    "        # Convert the network to a list of layers\n",
    "        layers = list(self.template_network)\n",
    "\n",
    "        # Find indicies for all linear layers\n",
    "        layer_indices = [i for i, layer in enumerate(self.template_network) if isinstance(layer, nn.Linear)]\n",
    "\n",
    "        # Exclude input and output layers\n",
    "        if (len(layer_indices) <= 2):\n",
    "               print(\"Not enough layers to modify (only input/output layers present)\")\n",
    "               return\n",
    "\n",
    "        hidden_layer_indices = layer_indices[1:-1] # Only hidden layers\n",
    "        \n",
    "        # Choose two random layers that are next to each other \n",
    "        selected_index_1 = random.choice(hidden_layer_indices)  # First layer\n",
    "        selected_index_2 = selected_index_1 + 2 # Second layer\n",
    "        \n",
    "        new_leak = snn.Leaky(beta=beta,spike_grad=spike_grad) # Creating a leaky along with synapses\n",
    "        \n",
    "        # Insert the new synapse between the two selected layers (selected_index_1 --> new_synapse --> selected_index_2)\n",
    "        # Get in_features of second layer and out_features of first layer\n",
    "        out_features_1 = layers[selected_index_1].out_features \n",
    "        in_features_2 = layers[selected_index_2].in_features\n",
    "\n",
    "        # Create new connection\n",
    "        new_synapse = nn.Linear(out_features_1, in_features_2)\n",
    "        \n",
    "        # Insert new layer between the selected layers\n",
    "        layers.insert(selected_index_2, new_synapse)\n",
    "        layers.insert(selected_index_2 + 1, new_leak)\n",
    "\n",
    "        print(f\"Added new synapse at {selected_index_2}.\")\n",
    "        \n",
    "        # Update the network with the new layers\n",
    "        self.template_network = nn.Sequential(*layers)\n",
    "\n",
    "    def remove_edge(self):\n",
    "        \"\"\"\n",
    "        Dynamically remove a synapse (new connection) between two randomly selected layers in the network.\n",
    "        This method removes a new nn.Linear layer between two existing layers.\n",
    "        \"\"\"\n",
    "        if self.template_network is None:\n",
    "            print(\"Template network not initialized\")\n",
    "            return\n",
    "        \n",
    "        # Convert the network to a list of layers\n",
    "        layers = list(self.template_network)\n",
    "\n",
    "        # Find indicies for all linear layers\n",
    "        layer_indices = [i for i, layer in enumerate(self.template_network) if isinstance(layer, nn.Linear)]\n",
    "\n",
    "        # Exclude input and output layers\n",
    "        if (len(layer_indices) <= 2):\n",
    "               print(\"Not enough layers to modify (only input/output layers present)\")\n",
    "               return\n",
    "\n",
    "        hidden_layer_indices = layer_indices[1:-1] # Only hidden layers\n",
    "        \n",
    "        # Choose a random hidden layer to delete\n",
    "        selected_index_1 = random.choice(hidden_layer_indices)  # Chosen layer\n",
    "        selected_index_2 = selected_index_1 + 1 # Associated leaky layer\n",
    "        \n",
    "        # Remove chosen layer\n",
    "        remove_layers = [selected_index_1, selected_index_2]\n",
    "        layers = [layer for index, layer in enumerate(layers) if index not in remove_layers]\n",
    "        \n",
    "        # Fix I/O between synapses\n",
    "        layer_indices = [i for i, layer in enumerate(layers) if isinstance(layer, nn.Linear)]\n",
    "\n",
    "        # Do not run for last index b/c there are no more synapses to check (index out of range)\n",
    "        for layer_idx in layer_indices[:-1]:\n",
    "             # Get the out features of the first layer and see if it matches the in features of the next layer\n",
    "             out_features = layers[layer_idx].out_features\n",
    "             in_features = layers[layer_idx+2].in_features\n",
    "\n",
    "             # Make in features of next layer equal out features of previous layer if they don't match\n",
    "             if (out_features != in_features):\n",
    "                modified_layer = nn.Linear(out_features, layers[layer_idx+2].out_features)\n",
    "\n",
    "                # Copy the existing weights and biases to the new layer\n",
    "                with torch.no_grad():\n",
    "                    modified_layer.weight.data = layers[layer_idx + 2].weight.data.clone()\n",
    "                    modified_layer.bias.data = layers[layer_idx + 2].bias.data.clone()\n",
    "\n",
    "\n",
    "                layers[layer_idx+2] = modified_layer\n",
    "\n",
    "        print(f\"Removed a synapse at {selected_index_1}.\")\n",
    "        \n",
    "        # Update the network with the new layers\n",
    "        self.template_network = nn.Sequential(*layers)\n",
    "\n",
    "    def update_edge_param(self):\n",
    "        \"\"\"\n",
    "        This method randomly selects a synapse (i.e., a connection between two neurons),\n",
    "        and updates the weight of that synapse.\n",
    "\n",
    "        NOTE: The input neuron's parameters aren't directly affected by the synapse update because the input neuron doesn't learn its own weight \n",
    "              — it only passes its value to the next layer. Thus, only the weight (synapse) and output neuron's bias are updated.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self.template_network is None:\n",
    "            print(\"Template network not initialized\")\n",
    "            return\n",
    "\n",
    "        # Convert the network to a list of layers\n",
    "        layers = list(self.template_network)\n",
    "\n",
    "        # Find all linear layers in the network\n",
    "        layer_indices = [i for i, layer in enumerate(layers) if isinstance(layer, nn.Linear)]\n",
    "\n",
    "        # Randomly select a layer (excluding the input and output layers)\n",
    "        selected_index = random.choice(layer_indices)\n",
    "        linear_layer = layers[selected_index]\n",
    "\n",
    "        # Ensure there are enough input and output neurons before proceeding\n",
    "        if linear_layer.in_features <= 1 or linear_layer.out_features <= 1:\n",
    "            print(f\"Layer at index {selected_index} does not have enough neurons. Skipping.\")\n",
    "            self.update_edge_param()\n",
    "\n",
    "        # Select random neuron indices for both the input and output neurons\n",
    "        input_neuron_index = random.choice(range(linear_layer.in_features))  # Random input neuron\n",
    "        output_neuron_index = random.choice(range(linear_layer.out_features))  # Random output neuron\n",
    "\n",
    "        #old_weight_input = linear_layer.weight[output_neuron_index, input_neuron_index].clone()\n",
    "\n",
    "        #old_weight_output = linear_layer.weight[output_neuron_index, input_neuron_index].clone()\n",
    "        #old_bias_output = linear_layer.bias[output_neuron_index].clone()\n",
    "\n",
    "        # Update the synapse weight (i.e., the connection between input neuron and output neuron)\n",
    "        new_weight = torch.rand(1)  # Random weight for the synapse\n",
    "        with torch.no_grad():  \n",
    "            # Accessing the specific weight between the two neurons and updating it\n",
    "            linear_layer.weight[output_neuron_index, input_neuron_index] = new_weight\n",
    "\n",
    "        # Update the bias for the both neurons\n",
    "        new_bias = torch.rand(1)\n",
    "        with torch.no_grad():  \n",
    "            linear_layer.bias[output_neuron_index] = new_bias\n",
    "\n",
    "        #print(\"Newly generated weight and bias\")\n",
    "        #print(new_weight)\n",
    "        #print(new_bias)\n",
    "\n",
    "\n",
    "        # Update the network with the new layers (though layers should already be updated)\n",
    "        self.template_network = nn.Sequential(*layers)\n",
    "\n",
    "        print(f\"Updated the synapse connecting input neuron {input_neuron_index} to output neuron {output_neuron_index} \"\n",
    "            f\"in layer {selected_index}.\")\n",
    "        \n",
    "        #return selected_index, input_neuron_index, old_weight_input, output_neuron_index, old_weight_output, old_bias_output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6b547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "hyperparameters = getHyperParameters(config)\n",
    "\n",
    "test_eons = EONS(hyperparameters)\n",
    "test_eons.make_template_network(50, 10)\n",
    "print(test_eons.template_network, end=\"\\n\\n\")\n",
    "test_eons.add_node()\n",
    "print(test_eons.template_network, end=\"\\n\\n\")\n",
    "test_eons.remove_node()\n",
    "print(test_eons.template_network, end=\"\\n\\n\")\n",
    "test_eons.update_node_param()\n",
    "print(\"\\n\")\n",
    "test_eons.add_edge()\n",
    "print(test_eons.template_network, end=\"\\n\\n\")\n",
    "test_eons.remove_edge()\n",
    "print(test_eons.template_network, end=\"\\n\\n\")\n",
    "test_eons.update_edge_param()\n",
    "print(\"\\n\")\n",
    "\n",
    "# UNCOMMENT THE LINES BELOW IF YOU WANT TO PRINT OUT RESULTS FROM UPDATE_NODE_PARAM() AND SEE UPDATES\n",
    "# layer_index, neuron_index, old_weights, old_bias = test_eons.update_node_param() \n",
    "# print(\"\\nWeight and Bias before the update\")\n",
    "# print(old_weights)\n",
    "# print(old_bias)\n",
    "\n",
    "# print(\"\\nWeight and Bias after the update\")\n",
    "# layers = list(test_eons.template_network)\n",
    "# print(layers[layer_index].weight[neuron_index])\n",
    "# print(layers[layer_index].bias[neuron_index])\n",
    "\n",
    "# UNCOMMENT THE LINES BELOW IF YOU WANT TO PRINT OUT RESULTS FROM UPDATE_EDGE_PARAM() AND SEE UPDATES\n",
    "# layer_index, input_neuron_index, old_weight_input, output_neuron_index, old_weight_output, old_bias_output = test_eons.update_edge_param()\n",
    "# print(\"\\nWeight and Bias before the update\")\n",
    "# print(\"Input Neuron:\")\n",
    "# print(f\"\\tWeight: {old_weight_input}\")\n",
    "# print(\"\\nOutput Neuron:\")\n",
    "# print(f\"\\tWeight: {old_weight_output}\")\n",
    "# print(f\"\\tBias: {old_bias_output}\")\n",
    "\n",
    "# print(\"\\nWeight and Bias after the update\")\n",
    "# layers = list(test_eons.template_network)\n",
    "# print(\"Input Neuron:\")\n",
    "# print(f\"\\tWeight: {layers[layer_index].weight[output_neuron_index, input_neuron_index]}\")\n",
    "# print(\"\\nOutput Neuron:\")\n",
    "# print(f\"\\tWeight: {layers[layer_index].weight[output_neuron_index, input_neuron_index]}\")\n",
    "# print(f\"\\tBias: {layers[layer_index].bias[output_neuron_index]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
